# app.py
import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import PromptTemplate

# === CONFIGURACI√ìN ===
DATA_DIR = "data"
INDEX_DIR = "data/faiss_index"
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

st.set_page_config(page_title="Agentic RAG con MultiQuery", page_icon="üß†")
st.title("üß† Agentic RAG ‚Äî Pol√≠ticas y Procedimientos")
st.caption("Consulta los documentos internos de tu empresa usando recuperaci√≥n sem√°ntica (RAG).")

# --- Construcci√≥n del √≠ndice ---
def build_index():
    pdf_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]
    if not pdf_files:
        st.error("No se encontraron archivos PDF en la carpeta 'data/'.")
        return None

    docs = []
    for pdf in pdf_files:
        st.write(f"üìÑ Cargando {pdf}...")
        loader = PyPDFLoader(os.path.join(DATA_DIR, pdf))
        docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=350)
    splits = splitter.split_documents(docs)
    st.write(f"‚úÖ Total de fragmentos: {len(splits)}")

    embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
    db = FAISS.from_documents(splits, embeddings)
    os.makedirs(INDEX_DIR, exist_ok=True)
    db.save_local(INDEX_DIR)
    st.success("üíæ √çndice FAISS reconstruido correctamente.")
    return db


# --- Carga del √≠ndice ---
@st.cache_resource
def load_vectorstore():
    if not os.path.exists(INDEX_DIR):
        st.warning("No se encontr√≥ un √≠ndice FAISS. Presiona el bot√≥n para generarlo.")
        return None
    try:
        embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
        vs = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception as e:
        st.error(f"‚ùå No se pudo cargar el √≠ndice: {e}")
        return None


# --- Botones de control ---
st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
if st.sidebar.button("üîÅ Reconstruir √≠ndice"):
    with st.spinner("Reconstruyendo √≠ndice..."):
        vs = build_index()
else:
    vs = load_vectorstore()

if st.sidebar.button("üóëÔ∏è Limpiar cach√© y recargar √≠ndice"):
    st.cache_resource.clear()
    st.success("üßπ Cach√© limpiada. Vuelve a preguntar para usar el √≠ndice actualizado.")

if vs is None:
    st.stop()

try:
    n_docs = len(vs.docstore._dict)
    st.info(f"üìö √çndice cargado con {n_docs} fragmentos de texto.")
except Exception:
    pass


# --- Configuraci√≥n del retriever mejorado ---
MQ_PROMPT_ES = PromptTemplate.from_template(
    "Genera 4 reformulaciones breves y diferentes, en espa√±ol, de la siguiente consulta del usuario.\n"
    "Usa sin√≥nimos y expresiones cercanas, sin traducir al ingl√©s.\n"
    "Solo devuelve una lista con una reformulaci√≥n por l√≠nea.\n\n"
    "Consulta: {question}"
)

def get_retriever(vectorstore):
    base = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 10, "fetch_k": 40})
    llm_q = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    return MultiQueryRetriever.from_llm(retriever=base, llm=llm_q, prompt=MQ_PROMPT_ES)


# --- Generaci√≥n de respuesta ---
def answer(question: str, vectorstore: FAISS) -> str:
    retriever = get_retriever(vectorstore)
    docs = retriever.get_relevant_documents(question)

    if not docs:
        return "No encuentro pasajes relevantes en los documentos cargados."

    context = "\n\n".join(d.page_content for d in docs)
    st.write("üîç **Vista previa del contexto recuperado:**")
    st.text(context[:1000] + "..." if len(context) > 1000 else context)

    prompt = f"""
Eres un asistente que responde exclusivamente seg√∫n las pol√≠ticas y procedimientos de la empresa.
Responde solo con base en el siguiente contexto. Si no hay informaci√≥n suficiente, responde "No tengo informaci√≥n suficiente en los documentos".

Pregunta: {question}

Contexto:
{context}
"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    return llm.predict(prompt).strip()


# --- Interfaz principal ---
q = st.text_input("‚úèÔ∏è Escribe tu consulta:", placeholder="Ejemplo: ¬øCu√°l es el procedimiento del seguro complementario?")
if st.button("Enviar") and q.strip():
    with st.spinner("Buscando informaci√≥n..."):
        resp = answer(q, vs)
    st.markdown(f"**Respuesta:**\n\n{resp}")